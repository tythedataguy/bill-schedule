{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "\n",
    "base_url = \"https://capitol.texas.gov\"\n",
    "urls = {\n",
    "    \"House\": f\"{base_url}/Committees/MeetingsUpcoming.aspx?Chamber=H\",\n",
    "    \"Senate\": f\"{base_url}/Committees/MeetingsUpcoming.aspx?Chamber=S\",\n",
    "}\n",
    "\n",
    "# Function to clean text\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "# Function to extract weekday\n",
    "def extract_weekday(date_text):\n",
    "    try:\n",
    "        match = re.search(r\"([A-Za-z]+ \\d{1,2}, \\d{4})\", date_text)\n",
    "        if match:\n",
    "            return datetime.strptime(match.group(1), \"%B %d, %Y\").strftime(\"%A\")\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Scrape data\n",
    "data = []\n",
    "for chamber, url in urls.items():\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    meeting_links = [base_url + a[\"href\"] for a in soup.find_all(\"a\", href=True) if a[\"href\"].endswith(\".HTM\")]\n",
    "\n",
    "    for link in meeting_links:\n",
    "        meeting_response = requests.get(link)\n",
    "        meeting_soup = BeautifulSoup(meeting_response.text, \"html.parser\")\n",
    "\n",
    "        committee_name = next((p.text.split(\":\")[-1].strip() for p in meeting_soup.find_all(\"p\") if \"COMMITTEE:\" in p.text), \"Unknown Committee\")\n",
    "        meeting_day = next((extract_weekday(p.text) for p in meeting_soup.find_all(\"p\") if \"TIME & DATE:\" in p.text), \"Unknown\")\n",
    "\n",
    "        for td in meeting_soup.find_all(\"td\"):\n",
    "            bill_link = td.find(\"a\")\n",
    "            if bill_link and \"Bill=\" in bill_link[\"href\"]:\n",
    "                bill_number = bill_link.text.strip()\n",
    "                full_text = td.get_text(\"\\n\").strip()\n",
    "                text_parts = list(filter(None, full_text.split(\"\\n\")))\n",
    "\n",
    "                bill_author = normalize_text(text_parts[1]) if len(text_parts) > 1 else \"Unknown\"\n",
    "                caption = \" \".join(text_parts[2:]).strip() if len(text_parts) > 2 else \"No Caption\"\n",
    "\n",
    "                data.append([chamber, meeting_day, committee_name, bill_number, bill_author, caption])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Chamber\", \"Day\", \"Committee Name\", \"Bill Number\", \"Bill Author\", \"Caption\"])\n",
    "df[\"Stance\"] = \"\"\n",
    "df.to_csv(\"bills.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
